{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Creditcard_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      1  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info and description of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 772 entries, 0 to 771\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Time    772 non-null    int64  \n",
      " 1   V1      772 non-null    float64\n",
      " 2   V2      772 non-null    float64\n",
      " 3   V3      772 non-null    float64\n",
      " 4   V4      772 non-null    float64\n",
      " 5   V5      772 non-null    float64\n",
      " 6   V6      772 non-null    float64\n",
      " 7   V7      772 non-null    float64\n",
      " 8   V8      772 non-null    float64\n",
      " 9   V9      772 non-null    float64\n",
      " 10  V10     772 non-null    float64\n",
      " 11  V11     772 non-null    float64\n",
      " 12  V12     772 non-null    float64\n",
      " 13  V13     772 non-null    float64\n",
      " 14  V14     772 non-null    float64\n",
      " 15  V15     772 non-null    float64\n",
      " 16  V16     772 non-null    float64\n",
      " 17  V17     772 non-null    float64\n",
      " 18  V18     772 non-null    float64\n",
      " 19  V19     772 non-null    float64\n",
      " 20  V20     772 non-null    float64\n",
      " 21  V21     772 non-null    float64\n",
      " 22  V22     772 non-null    float64\n",
      " 23  V23     772 non-null    float64\n",
      " 24  V24     772 non-null    float64\n",
      " 25  V25     772 non-null    float64\n",
      " 26  V26     772 non-null    float64\n",
      " 27  V27     772 non-null    float64\n",
      " 28  V28     772 non-null    float64\n",
      " 29  Amount  772 non-null    float64\n",
      " 30  Class   772 non-null    int64  \n",
      "dtypes: float64(29), int64(2)\n",
      "memory usage: 187.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>772.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>283.005181</td>\n",
       "      <td>-0.176963</td>\n",
       "      <td>0.217169</td>\n",
       "      <td>0.875172</td>\n",
       "      <td>0.285628</td>\n",
       "      <td>-0.005029</td>\n",
       "      <td>0.159081</td>\n",
       "      <td>0.123329</td>\n",
       "      <td>-0.057547</td>\n",
       "      <td>-0.030384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>-0.096995</td>\n",
       "      <td>-0.040344</td>\n",
       "      <td>-0.002501</td>\n",
       "      <td>0.114337</td>\n",
       "      <td>0.022782</td>\n",
       "      <td>0.023353</td>\n",
       "      <td>-0.017045</td>\n",
       "      <td>68.668290</td>\n",
       "      <td>0.011658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>171.834196</td>\n",
       "      <td>1.294724</td>\n",
       "      <td>1.173401</td>\n",
       "      <td>1.031878</td>\n",
       "      <td>1.258758</td>\n",
       "      <td>1.098143</td>\n",
       "      <td>1.225682</td>\n",
       "      <td>0.852075</td>\n",
       "      <td>0.830144</td>\n",
       "      <td>0.878183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609335</td>\n",
       "      <td>0.607228</td>\n",
       "      <td>0.358724</td>\n",
       "      <td>0.621507</td>\n",
       "      <td>0.429667</td>\n",
       "      <td>0.484227</td>\n",
       "      <td>0.300934</td>\n",
       "      <td>0.278332</td>\n",
       "      <td>197.838269</td>\n",
       "      <td>0.107411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.093248</td>\n",
       "      <td>-12.114213</td>\n",
       "      <td>-5.694973</td>\n",
       "      <td>-4.657545</td>\n",
       "      <td>-6.631951</td>\n",
       "      <td>-3.498447</td>\n",
       "      <td>-4.925568</td>\n",
       "      <td>-7.494658</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.134608</td>\n",
       "      <td>-2.776923</td>\n",
       "      <td>-3.553381</td>\n",
       "      <td>-1.867208</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>-1.243924</td>\n",
       "      <td>-2.377933</td>\n",
       "      <td>-2.735623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>126.500000</td>\n",
       "      <td>-0.896416</td>\n",
       "      <td>-0.174684</td>\n",
       "      <td>0.308677</td>\n",
       "      <td>-0.460058</td>\n",
       "      <td>-0.534567</td>\n",
       "      <td>-0.630717</td>\n",
       "      <td>-0.296289</td>\n",
       "      <td>-0.167880</td>\n",
       "      <td>-0.517068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213746</td>\n",
       "      <td>-0.525289</td>\n",
       "      <td>-0.176915</td>\n",
       "      <td>-0.379766</td>\n",
       "      <td>-0.166227</td>\n",
       "      <td>-0.313631</td>\n",
       "      <td>-0.047868</td>\n",
       "      <td>-0.033083</td>\n",
       "      <td>5.987500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>282.000000</td>\n",
       "      <td>-0.382618</td>\n",
       "      <td>0.285843</td>\n",
       "      <td>0.905435</td>\n",
       "      <td>0.395919</td>\n",
       "      <td>-0.116612</td>\n",
       "      <td>-0.109581</td>\n",
       "      <td>0.116329</td>\n",
       "      <td>0.034755</td>\n",
       "      <td>-0.082270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075802</td>\n",
       "      <td>-0.076551</td>\n",
       "      <td>-0.048353</td>\n",
       "      <td>0.091886</td>\n",
       "      <td>0.143723</td>\n",
       "      <td>-0.026414</td>\n",
       "      <td>0.023199</td>\n",
       "      <td>0.021034</td>\n",
       "      <td>16.665000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>432.000000</td>\n",
       "      <td>1.110739</td>\n",
       "      <td>0.885745</td>\n",
       "      <td>1.532969</td>\n",
       "      <td>1.117559</td>\n",
       "      <td>0.452818</td>\n",
       "      <td>0.482972</td>\n",
       "      <td>0.575390</td>\n",
       "      <td>0.252395</td>\n",
       "      <td>0.412261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095149</td>\n",
       "      <td>0.307438</td>\n",
       "      <td>0.070085</td>\n",
       "      <td>0.426339</td>\n",
       "      <td>0.425798</td>\n",
       "      <td>0.260408</td>\n",
       "      <td>0.112199</td>\n",
       "      <td>0.087023</td>\n",
       "      <td>55.527500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>581.000000</td>\n",
       "      <td>1.586093</td>\n",
       "      <td>5.267376</td>\n",
       "      <td>3.772857</td>\n",
       "      <td>4.075817</td>\n",
       "      <td>7.672544</td>\n",
       "      <td>5.122103</td>\n",
       "      <td>4.808426</td>\n",
       "      <td>2.134599</td>\n",
       "      <td>5.459274</td>\n",
       "      <td>...</td>\n",
       "      <td>5.273420</td>\n",
       "      <td>1.574750</td>\n",
       "      <td>3.150413</td>\n",
       "      <td>1.215279</td>\n",
       "      <td>1.136720</td>\n",
       "      <td>3.087444</td>\n",
       "      <td>2.490503</td>\n",
       "      <td>1.575380</td>\n",
       "      <td>3828.040000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time          V1          V2          V3          V4          V5  \\\n",
       "count  772.000000  772.000000  772.000000  772.000000  772.000000  772.000000   \n",
       "mean   283.005181   -0.176963    0.217169    0.875172    0.285628   -0.005029   \n",
       "std    171.834196    1.294724    1.173401    1.031878    1.258758    1.098143   \n",
       "min      0.000000   -6.093248  -12.114213   -5.694973   -4.657545   -6.631951   \n",
       "25%    126.500000   -0.896416   -0.174684    0.308677   -0.460058   -0.534567   \n",
       "50%    282.000000   -0.382618    0.285843    0.905435    0.395919   -0.116612   \n",
       "75%    432.000000    1.110739    0.885745    1.532969    1.117559    0.452818   \n",
       "max    581.000000    1.586093    5.267376    3.772857    4.075817    7.672544   \n",
       "\n",
       "               V6          V7          V8          V9  ...         V21  \\\n",
       "count  772.000000  772.000000  772.000000  772.000000  ...  772.000000   \n",
       "mean     0.159081    0.123329   -0.057547   -0.030384  ...    0.004888   \n",
       "std      1.225682    0.852075    0.830144    0.878183  ...    0.609335   \n",
       "min     -3.498447   -4.925568   -7.494658   -2.770089  ...   -4.134608   \n",
       "25%     -0.630717   -0.296289   -0.167880   -0.517068  ...   -0.213746   \n",
       "50%     -0.109581    0.116329    0.034755   -0.082270  ...   -0.075802   \n",
       "75%      0.482972    0.575390    0.252395    0.412261  ...    0.095149   \n",
       "max      5.122103    4.808426    2.134599    5.459274  ...    5.273420   \n",
       "\n",
       "              V22         V23         V24         V25         V26         V27  \\\n",
       "count  772.000000  772.000000  772.000000  772.000000  772.000000  772.000000   \n",
       "mean    -0.096995   -0.040344   -0.002501    0.114337    0.022782    0.023353   \n",
       "std      0.607228    0.358724    0.621507    0.429667    0.484227    0.300934   \n",
       "min     -2.776923   -3.553381   -1.867208   -1.389079   -1.243924   -2.377933   \n",
       "25%     -0.525289   -0.176915   -0.379766   -0.166227   -0.313631   -0.047868   \n",
       "50%     -0.076551   -0.048353    0.091886    0.143723   -0.026414    0.023199   \n",
       "75%      0.307438    0.070085    0.426339    0.425798    0.260408    0.112199   \n",
       "max      1.574750    3.150413    1.215279    1.136720    3.087444    2.490503   \n",
       "\n",
       "              V28       Amount       Class  \n",
       "count  772.000000   772.000000  772.000000  \n",
       "mean    -0.017045    68.668290    0.011658  \n",
       "std      0.278332   197.838269    0.107411  \n",
       "min     -2.735623     0.000000    0.000000  \n",
       "25%     -0.033083     5.987500    0.000000  \n",
       "50%      0.021034    16.665000    0.000000  \n",
       "75%      0.087023    55.527500    0.000000  \n",
       "max      1.575380  3828.040000    1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772, 31)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Class\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. of Unique values and its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    763\n",
       "1      9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar graph using seaborn of imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Bar Graph of Class column')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=data[\"Class\"].value_counts().index,y=data[\"Class\"].value_counts()[:])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('No. of Occurence')\n",
    "plt.title('Bar Graph of Class column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"Class\", axis = 1)\n",
    "y = data[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "# fit predictor and target variable\n",
    "X_new, y_new = smote.fit_resample(X, y)\n",
    "new_data=X_new.join(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    763\n",
       "1    763\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar graph after balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Bar Graph of Class column')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,12))\n",
    "sns.barplot(x=new_data[\"Class\"].value_counts().index,y=new_data[\"Class\"].value_counts()[:])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('No. of Occurence')\n",
    "plt.title('Bar Graph of Class column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.1599999999999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size=((1.96*1.96)*(0.5*0.5))/(0.05*0.05)\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>215</td>\n",
       "      <td>-0.176142</td>\n",
       "      <td>0.482386</td>\n",
       "      <td>0.553170</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>0.836403</td>\n",
       "      <td>0.176638</td>\n",
       "      <td>0.292566</td>\n",
       "      <td>0.139653</td>\n",
       "      <td>-0.102866</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135755</td>\n",
       "      <td>-0.355772</td>\n",
       "      <td>0.094130</td>\n",
       "      <td>-1.208778</td>\n",
       "      <td>-1.243871</td>\n",
       "      <td>0.098131</td>\n",
       "      <td>0.152277</td>\n",
       "      <td>0.145621</td>\n",
       "      <td>0.991378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>377</td>\n",
       "      <td>1.166919</td>\n",
       "      <td>0.027049</td>\n",
       "      <td>0.513875</td>\n",
       "      <td>0.860965</td>\n",
       "      <td>-0.519452</td>\n",
       "      <td>-0.681147</td>\n",
       "      <td>0.074992</td>\n",
       "      <td>-0.187776</td>\n",
       "      <td>0.345399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202750</td>\n",
       "      <td>-0.441391</td>\n",
       "      <td>-0.025782</td>\n",
       "      <td>0.452607</td>\n",
       "      <td>0.467223</td>\n",
       "      <td>0.262577</td>\n",
       "      <td>-0.023834</td>\n",
       "      <td>0.020521</td>\n",
       "      <td>40.830000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.680000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>284</td>\n",
       "      <td>-0.810756</td>\n",
       "      <td>0.654499</td>\n",
       "      <td>2.217257</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>-0.286801</td>\n",
       "      <td>0.117833</td>\n",
       "      <td>0.287552</td>\n",
       "      <td>-0.736461</td>\n",
       "      <td>0.699092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>0.571651</td>\n",
       "      <td>-0.101609</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>-0.170947</td>\n",
       "      <td>-0.471524</td>\n",
       "      <td>0.058958</td>\n",
       "      <td>-0.079157</td>\n",
       "      <td>30.300000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>346</td>\n",
       "      <td>1.077079</td>\n",
       "      <td>0.284980</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>1.657073</td>\n",
       "      <td>0.052020</td>\n",
       "      <td>0.446389</td>\n",
       "      <td>-0.407036</td>\n",
       "      <td>0.355704</td>\n",
       "      <td>0.626039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174337</td>\n",
       "      <td>-0.174161</td>\n",
       "      <td>-0.153375</td>\n",
       "      <td>-0.466331</td>\n",
       "      <td>0.611001</td>\n",
       "      <td>-0.252871</td>\n",
       "      <td>0.090375</td>\n",
       "      <td>0.054820</td>\n",
       "      <td>10.990000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "1361   215 -0.176142  0.482386  0.553170  0.008663  0.836403  0.176638   \n",
       "511    377  1.166919  0.027049  0.513875  0.860965 -0.519452 -0.681147   \n",
       "9        9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "393    284 -0.810756  0.654499  2.217257  0.104341 -0.286801  0.117833   \n",
       "471    346  1.077079  0.284980  0.007731  1.657073  0.052020  0.446389   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "1361  0.292566  0.139653 -0.102866  ... -0.135755 -0.355772  0.094130   \n",
       "511   0.074992 -0.187776  0.345399  ... -0.202750 -0.441391 -0.025782   \n",
       "9     0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794   \n",
       "393   0.287552 -0.736461  0.699092  ...  0.938194  0.571651 -0.101609   \n",
       "471  -0.407036  0.355704  0.626039  ... -0.174337 -0.174161 -0.153375   \n",
       "\n",
       "           V24       V25       V26       V27       V28     Amount  Class  \n",
       "1361 -1.208778 -1.243871  0.098131  0.152277  0.145621   0.991378      1  \n",
       "511   0.452607  0.467223  0.262577 -0.023834  0.020521  40.830000      0  \n",
       "9    -0.385050 -0.069733  0.094199  0.246219  0.083076   3.680000      0  \n",
       "393   0.363928 -0.170947 -0.471524  0.058958 -0.079157  30.300000      0  \n",
       "471  -0.466331  0.611001 -0.252871  0.090375  0.054820  10.990000      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "random_sample = new_data.sample(n=math.ceil(sample_size),random_state=0,replace=False)\n",
    "random_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>29</td>\n",
       "      <td>1.110880</td>\n",
       "      <td>0.168717</td>\n",
       "      <td>0.517144</td>\n",
       "      <td>1.325407</td>\n",
       "      <td>-0.191573</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>-0.031849</td>\n",
       "      <td>0.117620</td>\n",
       "      <td>0.017665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037709</td>\n",
       "      <td>0.095701</td>\n",
       "      <td>-0.048198</td>\n",
       "      <td>0.232115</td>\n",
       "      <td>0.606201</td>\n",
       "      <td>-0.342097</td>\n",
       "      <td>0.036770</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>6.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>50</td>\n",
       "      <td>-0.571521</td>\n",
       "      <td>1.071600</td>\n",
       "      <td>1.280110</td>\n",
       "      <td>0.542780</td>\n",
       "      <td>0.574439</td>\n",
       "      <td>-0.259359</td>\n",
       "      <td>1.061148</td>\n",
       "      <td>-0.410972</td>\n",
       "      <td>-0.179130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.561240</td>\n",
       "      <td>-0.199287</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>-0.179530</td>\n",
       "      <td>-0.374116</td>\n",
       "      <td>0.071641</td>\n",
       "      <td>-0.175510</td>\n",
       "      <td>9.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>76</td>\n",
       "      <td>-1.024576</td>\n",
       "      <td>0.522289</td>\n",
       "      <td>1.787699</td>\n",
       "      <td>0.202672</td>\n",
       "      <td>-1.140803</td>\n",
       "      <td>-0.137831</td>\n",
       "      <td>-0.336555</td>\n",
       "      <td>0.670704</td>\n",
       "      <td>0.071670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315868</td>\n",
       "      <td>0.847565</td>\n",
       "      <td>0.148877</td>\n",
       "      <td>0.549791</td>\n",
       "      <td>-0.585131</td>\n",
       "      <td>0.325841</td>\n",
       "      <td>-0.068871</td>\n",
       "      <td>0.059713</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>98</td>\n",
       "      <td>-0.646513</td>\n",
       "      <td>1.004199</td>\n",
       "      <td>1.616224</td>\n",
       "      <td>-0.099628</td>\n",
       "      <td>-0.122477</td>\n",
       "      <td>-0.671327</td>\n",
       "      <td>0.656183</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>-0.635963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147934</td>\n",
       "      <td>-0.420046</td>\n",
       "      <td>0.061424</td>\n",
       "      <td>0.520997</td>\n",
       "      <td>-0.238845</td>\n",
       "      <td>0.030135</td>\n",
       "      <td>0.140481</td>\n",
       "      <td>0.101163</td>\n",
       "      <td>14.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0       0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "39     29  1.110880  0.168717  0.517144  1.325407 -0.191573  0.019504   \n",
       "78     50 -0.571521  1.071600  1.280110  0.542780  0.574439 -0.259359   \n",
       "117    76 -1.024576  0.522289  1.787699  0.202672 -1.140803 -0.137831   \n",
       "156    98 -0.646513  1.004199  1.616224 -0.099628 -0.122477 -0.671327   \n",
       "\n",
       "           V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0    0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "39  -0.031849  0.117620  0.017665  ... -0.037709  0.095701 -0.048198   \n",
       "78   1.061148 -0.410972 -0.179130  ...  0.003559  0.561240 -0.199287   \n",
       "117 -0.336555  0.670704  0.071670  ...  0.315868  0.847565  0.148877   \n",
       "156  0.656183  0.009755 -0.635963  ... -0.147934 -0.420046  0.061424   \n",
       "\n",
       "          V24       V25       V26       V27       V28  Amount  Class  \n",
       "0    0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "39   0.232115  0.606201 -0.342097  0.036770  0.007480    6.54      0  \n",
       "78   0.001387 -0.179530 -0.374116  0.071641 -0.175510    9.79      0  \n",
       "117  0.549791 -0.585131  0.325841 -0.068871  0.059713   50.00      0  \n",
       "156  0.520997 -0.238845  0.030135  0.140481  0.101163   14.98      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = new_data.shape[0]\n",
    "k = int(math.sqrt(n))\n",
    "systematic_sample = new_data.iloc[::k]\n",
    "systematic_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>162</td>\n",
       "      <td>0.105165</td>\n",
       "      <td>0.545652</td>\n",
       "      <td>0.447886</td>\n",
       "      <td>0.130462</td>\n",
       "      <td>0.790929</td>\n",
       "      <td>0.216050</td>\n",
       "      <td>0.290702</td>\n",
       "      <td>0.130320</td>\n",
       "      <td>-0.118809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394015</td>\n",
       "      <td>0.150397</td>\n",
       "      <td>-1.318307</td>\n",
       "      <td>-1.346055</td>\n",
       "      <td>0.075929</td>\n",
       "      <td>0.224911</td>\n",
       "      <td>0.224829</td>\n",
       "      <td>1.035570</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>327</td>\n",
       "      <td>1.162687</td>\n",
       "      <td>0.269139</td>\n",
       "      <td>0.163258</td>\n",
       "      <td>0.943088</td>\n",
       "      <td>-0.003225</td>\n",
       "      <td>-0.421822</td>\n",
       "      <td>0.213395</td>\n",
       "      <td>-0.125542</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236577</td>\n",
       "      <td>-0.191415</td>\n",
       "      <td>0.042338</td>\n",
       "      <td>0.720315</td>\n",
       "      <td>-0.332043</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>36.710000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>503</td>\n",
       "      <td>0.074738</td>\n",
       "      <td>-3.032009</td>\n",
       "      <td>-0.429919</td>\n",
       "      <td>-0.442721</td>\n",
       "      <td>-1.784710</td>\n",
       "      <td>-0.066668</td>\n",
       "      <td>0.098736</td>\n",
       "      <td>-0.222525</td>\n",
       "      <td>-0.241858</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.247242</td>\n",
       "      <td>-0.620402</td>\n",
       "      <td>-0.368545</td>\n",
       "      <td>-0.073359</td>\n",
       "      <td>1.296835</td>\n",
       "      <td>-0.192760</td>\n",
       "      <td>0.121440</td>\n",
       "      <td>712.820000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>497</td>\n",
       "      <td>-1.128989</td>\n",
       "      <td>0.312565</td>\n",
       "      <td>1.603790</td>\n",
       "      <td>-0.025374</td>\n",
       "      <td>0.955043</td>\n",
       "      <td>-0.749995</td>\n",
       "      <td>0.727829</td>\n",
       "      <td>-0.082916</td>\n",
       "      <td>-0.101074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164030</td>\n",
       "      <td>-0.258936</td>\n",
       "      <td>0.230993</td>\n",
       "      <td>0.211121</td>\n",
       "      <td>-0.281785</td>\n",
       "      <td>-0.164107</td>\n",
       "      <td>-0.189667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>517</td>\n",
       "      <td>-1.718573</td>\n",
       "      <td>-1.734629</td>\n",
       "      <td>2.276252</td>\n",
       "      <td>0.888322</td>\n",
       "      <td>2.068755</td>\n",
       "      <td>0.201101</td>\n",
       "      <td>-1.324212</td>\n",
       "      <td>0.539888</td>\n",
       "      <td>0.669335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955065</td>\n",
       "      <td>0.151695</td>\n",
       "      <td>-0.530409</td>\n",
       "      <td>-0.058815</td>\n",
       "      <td>0.515290</td>\n",
       "      <td>-0.119318</td>\n",
       "      <td>-0.159151</td>\n",
       "      <td>1.368532</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "1095   162  0.105165  0.545652  0.447886  0.130462  0.790929  0.216050   \n",
       "449    327  1.162687  0.269139  0.163258  0.943088 -0.003225 -0.421822   \n",
       "666    503  0.074738 -3.032009 -0.429919 -0.442721 -1.784710 -0.066668   \n",
       "853    497 -1.128989  0.312565  1.603790 -0.025374  0.955043 -0.749995   \n",
       "1414   517 -1.718573 -1.734629  2.276252  0.888322  2.068755  0.201101   \n",
       "\n",
       "            V7        V8        V9  ...       V22       V23       V24  \\\n",
       "1095  0.290702  0.130320 -0.118809  ... -0.394015  0.150397 -1.318307   \n",
       "449   0.213395 -0.125542 -0.396783  ...  0.236577 -0.191415  0.042338   \n",
       "666   0.098736 -0.222525 -0.241858  ... -1.247242 -0.620402 -0.368545   \n",
       "853   0.727829 -0.082916 -0.101074  ...  0.164030 -0.258936  0.230993   \n",
       "1414 -1.324212  0.539888  0.669335  ...  0.955065  0.151695 -0.530409   \n",
       "\n",
       "           V25       V26       V27       V28      Amount  Class  cluster  \n",
       "1095 -1.346055  0.075929  0.224911  0.224829    1.035570      1      2.0  \n",
       "449   0.720315 -0.332043  0.009356  0.014457   36.710000      0      2.0  \n",
       "666  -0.073359  1.296835 -0.192760  0.121440  712.820000      0      2.0  \n",
       "853   0.211121 -0.281785 -0.164107 -0.189667    1.000000      1      2.0  \n",
       "1414 -0.058815  0.515290 -0.119318 -0.159151    1.368532      1      2.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_clustered_sample(data, n_per_cluster, num_select_clusters):\n",
    "    N = len(data)\n",
    "    K = int(N / n_per_cluster)\n",
    "    df = None\n",
    "\n",
    "    for k in range(K):\n",
    "        sample_k = data.sample(n=n_per_cluster)\n",
    "        sample_k[\"cluster\"] = np.repeat(k, len(sample_k))\n",
    "        data = data.drop(index=sample_k.index)\n",
    "        data = pd.concat([data, sample_k], axis=0)\n",
    "\n",
    "    random_chosen_clusters = np.random.choice(np.arange(K), size=num_select_clusters, replace=False)\n",
    "    samples = data[data.cluster.isin(random_chosen_clusters)]\n",
    "    return samples\n",
    "\n",
    "# Assuming 'data_new' is your DataFrame and 'n' is defined\n",
    "cluster_sample = get_clustered_sample(data=new_data, n_per_cluster=int(sample_size), num_select_clusters=1)\n",
    "cluster_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>517</td>\n",
       "      <td>1.314713</td>\n",
       "      <td>-0.328688</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>-0.805044</td>\n",
       "      <td>-0.467260</td>\n",
       "      <td>-0.522747</td>\n",
       "      <td>-0.180850</td>\n",
       "      <td>-0.093472</td>\n",
       "      <td>1.795353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122540</td>\n",
       "      <td>-0.029521</td>\n",
       "      <td>-0.250848</td>\n",
       "      <td>-0.427629</td>\n",
       "      <td>0.917790</td>\n",
       "      <td>-0.534370</td>\n",
       "      <td>0.062355</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>417</td>\n",
       "      <td>-2.680348</td>\n",
       "      <td>1.872052</td>\n",
       "      <td>1.144712</td>\n",
       "      <td>-0.693664</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.601325</td>\n",
       "      <td>0.904201</td>\n",
       "      <td>-0.520079</td>\n",
       "      <td>3.013065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459592</td>\n",
       "      <td>0.485421</td>\n",
       "      <td>-0.365437</td>\n",
       "      <td>-0.744118</td>\n",
       "      <td>0.328655</td>\n",
       "      <td>0.457695</td>\n",
       "      <td>0.566152</td>\n",
       "      <td>0.168241</td>\n",
       "      <td>29.990000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>495</td>\n",
       "      <td>-1.736342</td>\n",
       "      <td>-2.264512</td>\n",
       "      <td>2.291458</td>\n",
       "      <td>1.103778</td>\n",
       "      <td>2.232036</td>\n",
       "      <td>0.459165</td>\n",
       "      <td>-1.931870</td>\n",
       "      <td>0.707899</td>\n",
       "      <td>0.883843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364823</td>\n",
       "      <td>1.030789</td>\n",
       "      <td>0.284059</td>\n",
       "      <td>-0.759990</td>\n",
       "      <td>-0.183994</td>\n",
       "      <td>0.798822</td>\n",
       "      <td>-0.116403</td>\n",
       "      <td>-0.154669</td>\n",
       "      <td>1.596584</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>569</td>\n",
       "      <td>0.842164</td>\n",
       "      <td>0.321468</td>\n",
       "      <td>0.428988</td>\n",
       "      <td>0.503783</td>\n",
       "      <td>-0.180489</td>\n",
       "      <td>-0.959901</td>\n",
       "      <td>0.118516</td>\n",
       "      <td>-0.161630</td>\n",
       "      <td>0.015704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271973</td>\n",
       "      <td>-0.733475</td>\n",
       "      <td>0.073638</td>\n",
       "      <td>0.262604</td>\n",
       "      <td>0.143987</td>\n",
       "      <td>0.114630</td>\n",
       "      <td>-0.066977</td>\n",
       "      <td>-0.026544</td>\n",
       "      <td>1.249780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>574</td>\n",
       "      <td>-0.402057</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>2.474227</td>\n",
       "      <td>0.929684</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.297490</td>\n",
       "      <td>0.715195</td>\n",
       "      <td>-0.257153</td>\n",
       "      <td>0.593868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072812</td>\n",
       "      <td>0.445733</td>\n",
       "      <td>-0.245103</td>\n",
       "      <td>0.421234</td>\n",
       "      <td>0.049280</td>\n",
       "      <td>-0.388323</td>\n",
       "      <td>-0.329333</td>\n",
       "      <td>-0.386747</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "684    517  1.314713 -0.328688  0.002645 -0.805044 -0.467260 -0.522747   \n",
       "559    417 -2.680348  1.872052  1.144712 -0.693664  0.155172  0.601325   \n",
       "1216   495 -1.736342 -2.264512  2.291458  1.103778  2.232036  0.459165   \n",
       "835    569  0.842164  0.321468  0.428988  0.503783 -0.180489 -0.959901   \n",
       "763    574 -0.402057  0.584300  2.474227  0.929684  0.014314  0.297490   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "684  -0.180850 -0.093472  1.795353  ... -0.122540 -0.029521 -0.250848   \n",
       "559   0.904201 -0.520079  3.013065  ... -0.459592  0.485421 -0.365437   \n",
       "1216 -1.931870  0.707899  0.883843  ...  0.364823  1.030789  0.284059   \n",
       "835   0.118516 -0.161630  0.015704  ... -0.271973 -0.733475  0.073638   \n",
       "763   0.715195 -0.257153  0.593868  ... -0.072812  0.445733 -0.245103   \n",
       "\n",
       "           V24       V25       V26       V27       V28     Amount  Class  \n",
       "684  -0.427629  0.917790 -0.534370  0.062355  0.012093  10.000000      0  \n",
       "559  -0.744118  0.328655  0.457695  0.566152  0.168241  29.990000      0  \n",
       "1216 -0.759990 -0.183994  0.798822 -0.116403 -0.154669   1.596584      1  \n",
       "835   0.262604  0.143987  0.114630 -0.066977 -0.026544   1.249780      1  \n",
       "763   0.421234  0.049280 -0.388323 -0.329333 -0.386747  12.000000      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_sample = new_data.sample(n = math.ceil(sample_size), replace = True, random_state = 0)\n",
    "bootstrap_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>620</th>\n",
       "      <td>471</td>\n",
       "      <td>1.377497</td>\n",
       "      <td>-0.662565</td>\n",
       "      <td>-0.130149</td>\n",
       "      <td>-0.562680</td>\n",
       "      <td>-0.832895</td>\n",
       "      <td>-0.910754</td>\n",
       "      <td>-0.327043</td>\n",
       "      <td>-0.201709</td>\n",
       "      <td>-0.523053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133023</td>\n",
       "      <td>0.287921</td>\n",
       "      <td>-0.224325</td>\n",
       "      <td>0.052896</td>\n",
       "      <td>0.775265</td>\n",
       "      <td>-0.042445</td>\n",
       "      <td>-0.026107</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>260</td>\n",
       "      <td>1.026702</td>\n",
       "      <td>-0.661665</td>\n",
       "      <td>0.897601</td>\n",
       "      <td>0.144403</td>\n",
       "      <td>-1.105205</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>-0.707267</td>\n",
       "      <td>0.195918</td>\n",
       "      <td>0.743371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150661</td>\n",
       "      <td>0.322398</td>\n",
       "      <td>-0.154736</td>\n",
       "      <td>0.097655</td>\n",
       "      <td>0.174639</td>\n",
       "      <td>1.103205</td>\n",
       "      <td>-0.060709</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>99.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>137</td>\n",
       "      <td>0.753308</td>\n",
       "      <td>-1.131593</td>\n",
       "      <td>0.878730</td>\n",
       "      <td>0.297871</td>\n",
       "      <td>-1.327250</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>-0.535276</td>\n",
       "      <td>0.171133</td>\n",
       "      <td>1.184602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031912</td>\n",
       "      <td>-0.213298</td>\n",
       "      <td>-0.091660</td>\n",
       "      <td>0.151837</td>\n",
       "      <td>-0.067673</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>-0.063985</td>\n",
       "      <td>0.049174</td>\n",
       "      <td>226.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>-2.008872</td>\n",
       "      <td>2.198527</td>\n",
       "      <td>0.144242</td>\n",
       "      <td>1.159432</td>\n",
       "      <td>-0.815174</td>\n",
       "      <td>0.182288</td>\n",
       "      <td>-0.617108</td>\n",
       "      <td>1.530817</td>\n",
       "      <td>-0.586832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094917</td>\n",
       "      <td>0.294983</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.015249</td>\n",
       "      <td>0.034211</td>\n",
       "      <td>-0.236141</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.117986</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>241</td>\n",
       "      <td>-0.849348</td>\n",
       "      <td>1.019508</td>\n",
       "      <td>1.517236</td>\n",
       "      <td>0.579468</td>\n",
       "      <td>0.539187</td>\n",
       "      <td>-0.211665</td>\n",
       "      <td>1.346509</td>\n",
       "      <td>-0.989694</td>\n",
       "      <td>0.904159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.508111</td>\n",
       "      <td>-0.180737</td>\n",
       "      <td>0.017425</td>\n",
       "      <td>-0.525662</td>\n",
       "      <td>-0.480661</td>\n",
       "      <td>-1.187697</td>\n",
       "      <td>-0.654816</td>\n",
       "      <td>12.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time        V1        V2        V3        V4        V5        V6  \\\n",
       "Class                                                                         \n",
       "0     620   471  1.377497 -0.662565 -0.130149 -0.562680 -0.832895 -0.910754   \n",
       "      354   260  1.026702 -0.661665  0.897601  0.144403 -1.105205  0.020245   \n",
       "      206   137  0.753308 -1.131593  0.878730  0.297871 -1.327250  0.066596   \n",
       "      41     32 -2.008872  2.198527  0.144242  1.159432 -0.815174  0.182288   \n",
       "      330   241 -0.849348  1.019508  1.517236  0.579468  0.539187 -0.211665   \n",
       "\n",
       "                 V7        V8        V9  ...       V21       V22       V23  \\\n",
       "Class                                    ...                                 \n",
       "0     620 -0.327043 -0.201709 -0.523053  ...  0.133023  0.287921 -0.224325   \n",
       "      354 -0.707267  0.195918  0.743371  ...  0.150661  0.322398 -0.154736   \n",
       "      206 -0.535276  0.171133  1.184602  ...  0.031912 -0.213298 -0.091660   \n",
       "      41  -0.617108  1.530817 -0.586832  ...  0.094917  0.294983  0.011081   \n",
       "      330  1.346509 -0.989694  0.904159  ...  0.004282  0.508111 -0.180737   \n",
       "\n",
       "                V24       V25       V26       V27       V28  Amount  Class  \n",
       "Class                                                                       \n",
       "0     620  0.052896  0.775265 -0.042445 -0.026107  0.003801   48.00      0  \n",
       "      354  0.097655  0.174639  1.103205 -0.060709  0.016598   99.00      0  \n",
       "      206  0.151837 -0.067673  0.999073 -0.063985  0.049174  226.07      0  \n",
       "      41   0.015249  0.034211 -0.236141  0.128291  0.117986    2.35      0  \n",
       "      330  0.017425 -0.525662 -0.480661 -1.187697 -0.654816   12.20      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratified_sample = new_data.groupby(new_data[\"Class\"]).apply(lambda x: x.sample(n=math.ceil(sample_size), random_state=0))\n",
    "\n",
    "stratified_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffrent Models(XGB,Random Forest, Decision Tree, Logistic Regreesion, LGBM ) on Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Simple Random Sampling using XG-Boost: 0.9655172413793104\n",
      "Accuracy of Simple Random Sampling using Random_forest: 0.9827586206896551\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy of Simple Random Sampling using LGBM: 0.9741379310344828\n",
      "Accuracy of Simple Random Sampling using decision_tree: 0.9568965517241379\n",
      "Accuracy for simple random sampling using Logistic_Regression: 0.8879310344827587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "X_random_sample = random_sample.drop('Class', axis = 1)\n",
    "y_random_sample = random_sample['Class']\n",
    "X_train_random, X_test_random, y_train_random, y_test_random = train_test_split(X_random_sample, y_random_sample,train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train_random, y_train_random)\n",
    "y_pred = model.predict(X_test_random)\n",
    "accuracy_xg = accuracy_score(y_test_random, y_pred)\n",
    "print(\"Accuracy of Simple Random Sampling using XG-Boost:\", accuracy_xg)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "model.fit(X_train_random, y_train_random)\n",
    "y_pred = model.predict(X_test_random)\n",
    "accuracy_rf = accuracy_score(y_test_random, y_pred)\n",
    "print(\"Accuracy of Simple Random Sampling using Random_forest:\", accuracy_rf)\n",
    "\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMClassifier(verbose=0,force_col_wise=True)\n",
    "model.fit(X_train_random, y_train_random)\n",
    "y_pred = model.predict(X_test_random)\n",
    "accuracy_lgbm = accuracy_score(y_test_random, y_pred)\n",
    "print(\"Accuracy of Simple Random Sampling using LGBM:\", accuracy_lgbm)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model= DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train_random, y_train_random)\n",
    "y_pred = model.predict(X_test_random)\n",
    "accuracy_dt = accuracy_score(y_test_random, y_pred)\n",
    "print(\"Accuracy of Simple Random Sampling using decision_tree:\", accuracy_dt)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_random, y_train_random)\n",
    "y_pred = model.predict(X_test_random)\n",
    "accuracy_lr = accuracy_score(y_test_random, y_pred)\n",
    "print(\"Accuracy for simple random sampling using Logistic_Regression:\", accuracy_lr)\n",
    "\n",
    "list1=[accuracy_lr,accuracy_dt,accuracy_lgbm,accuracy_rf,accuracy_xg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffrent Models(XGB,Random Forest, Decision Tree, Logistic Regreesion, LGBM ) on systematic sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of systematic Sampling using XG-Boost: 0.8333333333333334\n",
      "Accuracy of systematic Sampling using Random_forest: 0.8333333333333334\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Accuracy of Simple Random Sampling using LGBM: 0.4166666666666667\n",
      "Accuracy of systematic Sampling using decision_tree: 1.0\n",
      "Accuracy for systematic sampling using Logistic_Regression: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_systematic_sample = systematic_sample.drop('Class', axis = 1)\n",
    "y_systematic_sample = systematic_sample['Class']\n",
    "X_train_systematic, X_test_systematic, y_train_systematic, y_test_systematic = train_test_split(X_systematic_sample, y_systematic_sample, test_size=0.3, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train_systematic, y_train_systematic)\n",
    "y_pred = model.predict(X_test_systematic)\n",
    "accuracy_xg = accuracy_score(y_test_systematic, y_pred)\n",
    "print(\"Accuracy of systematic Sampling using XG-Boost:\", accuracy_xg)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "model.fit(X_train_systematic, y_train_systematic)\n",
    "y_pred = model.predict(X_test_systematic)\n",
    "accuracy_rf = accuracy_score(y_test_systematic, y_pred)\n",
    "print(\"Accuracy of systematic Sampling using Random_forest:\", accuracy_rf)\n",
    "\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMClassifier(verbose=0,force_col_wise=True)\n",
    "model.fit(X_train_systematic, y_train_systematic)\n",
    "y_pred = model.predict(X_test_systematic)\n",
    "accuracy_lgbm = accuracy_score(y_test_systematic, y_pred)\n",
    "print(\"Accuracy of Simple Random Sampling using LGBM:\", accuracy_lgbm)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model= DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train_systematic, y_train_systematic)\n",
    "y_pred = model.predict(X_test_systematic)\n",
    "accuracy_dt = accuracy_score(y_test_systematic, y_pred)\n",
    "print(\"Accuracy of systematic Sampling using decision_tree:\", accuracy_dt)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_systematic, y_train_systematic)\n",
    "y_pred = model.predict(X_test_systematic)\n",
    "accuracy_lr = accuracy_score(y_test_systematic, y_pred)\n",
    "print(\"Accuracy for systematic sampling using Logistic_Regression:\", accuracy_lr)\n",
    "\n",
    "list2=[accuracy_lr,accuracy_dt,accuracy_lgbm,accuracy_rf,accuracy_xg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffrent Models(XGB,Random Forest, Decision Tree, Logistic Regreesion, LGBM ) on Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of stratified Sampling using XG-Boost: 0.9653679653679653\n",
      "Accuracy of stratified Sampling using Random_forest: 0.987012987012987\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy of Simple Random Sampling using LGBM: 0.987012987012987\n",
      "Accuracy of stratified Sampling using decision_tree: 0.9523809523809523\n",
      "Accuracy for stratified sampling using Logistic_Regression: 0.8917748917748918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_stratified_sample = stratified_sample.drop('Class', axis = 1)\n",
    "y_stratified_sample = stratified_sample['Class']\n",
    "X_train_stratified, X_test_stratified, y_train_stratified, y_test_stratified = train_test_split(X_stratified_sample, y_stratified_sample, test_size=0.3, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train_stratified, y_train_stratified)\n",
    "y_pred = model.predict(X_test_stratified)\n",
    "accuracy_xg = accuracy_score(y_test_stratified, y_pred)\n",
    "print(\"Accuracy of stratified Sampling using XG-Boost:\", accuracy_xg)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "model.fit(X_train_stratified, y_train_stratified)\n",
    "y_pred = model.predict(X_test_stratified)\n",
    "accuracy_rf = accuracy_score(y_test_stratified, y_pred)\n",
    "print(\"Accuracy of stratified Sampling using Random_forest:\", accuracy_rf)\n",
    "\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMClassifier(verbose=0,force_col_wise=True)\n",
    "model.fit(X_train_stratified, y_train_stratified)\n",
    "y_pred = model.predict(X_test_stratified)\n",
    "accuracy_lgbm = accuracy_score(y_test_stratified, y_pred)\n",
    "print(\"Accuracy of Simple Random Sampling using LGBM:\", accuracy_lgbm)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model= DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train_stratified, y_train_stratified)\n",
    "y_pred = model.predict(X_test_stratified)\n",
    "accuracy_dt = accuracy_score(y_test_stratified, y_pred)\n",
    "print(\"Accuracy of stratified Sampling using decision_tree:\", accuracy_dt)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_stratified, y_train_stratified)\n",
    "y_pred = model.predict(X_test_stratified)\n",
    "accuracy_lr = accuracy_score(y_test_stratified, y_pred)\n",
    "print(\"Accuracy for stratified sampling using Logistic_Regression:\", accuracy_lr)\n",
    "\n",
    "list3=[accuracy_lr,accuracy_dt,accuracy_lgbm,accuracy_rf,accuracy_xg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffrent Models(XGB,Random Forest, Decision Tree, Logistic Regreesion, LGBM ) on Cluster sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of cluster Sampling using XG-Boost: 0.9655172413793104\n",
      "Accuracy of cluster Sampling using Random_forest: 0.9913793103448276\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy of Simple Random Sampling using LGBM: 0.9913793103448276\n",
      "Accuracy of cluster Sampling using decision_tree: 0.9655172413793104\n",
      "Accuracy for cluster sampling using Logistic_Regression: 0.9482758620689655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_cluster_sample = cluster_sample.drop('Class', axis = 1)\n",
    "y_cluster_sample = cluster_sample['Class']\n",
    "X_train_cluster, X_test_cluster, y_train_cluster, y_test_cluster = train_test_split(X_cluster_sample, y_cluster_sample, test_size=0.3, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train_cluster, y_train_cluster)\n",
    "y_pred = model.predict(X_test_cluster)\n",
    "accuracy_xg = accuracy_score(y_test_cluster, y_pred)\n",
    "print(\"Accuracy of cluster Sampling using XG-Boost:\", accuracy_xg)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "model.fit(X_train_cluster, y_train_cluster)\n",
    "y_pred = model.predict(X_test_cluster)\n",
    "accuracy_rf = accuracy_score(y_test_cluster, y_pred)\n",
    "print(\"Accuracy of cluster Sampling using Random_forest:\", accuracy_rf)\n",
    "\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMClassifier(verbose=0,force_col_wise=True)\n",
    "model.fit(X_train_cluster, y_train_cluster)\n",
    "y_pred = model.predict(X_test_cluster)\n",
    "accuracy_lgbm = accuracy_score(y_test_cluster, y_pred)\n",
    "print(\"Accuracy of Simple Random Sampling using LGBM:\", accuracy_lgbm)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model= DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train_cluster, y_train_cluster)\n",
    "y_pred = model.predict(X_test_cluster)\n",
    "accuracy_dt = accuracy_score(y_test_cluster, y_pred)\n",
    "print(\"Accuracy of cluster Sampling using decision_tree:\", accuracy_dt)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_cluster, y_train_cluster)\n",
    "y_pred = model.predict(X_test_cluster)\n",
    "accuracy_lr = accuracy_score(y_test_cluster, y_pred)\n",
    "print(\"Accuracy for cluster sampling using Logistic_Regression:\", accuracy_lr)\n",
    "\n",
    "list4=[accuracy_lr,accuracy_dt,accuracy_lgbm,accuracy_rf,accuracy_xg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffrent Models(XGB,Random Forest, Decision Tree, Logistic Regreesion, LGBM ) on Bootstrap sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bootstrap Sampling using XG-Boost: 0.9482758620689655\n",
      "Accuracy of bootstrap Sampling using Random_forest: 0.9913793103448276\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy of Simple Random Sampling using LGBM: 0.9741379310344828\n",
      "Accuracy of bootstrap Sampling using decision_tree: 0.9741379310344828\n",
      "Accuracy for bootstrap sampling using Logistic_Regression: 0.9310344827586207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_bootstrap_sample = bootstrap_sample.drop('Class', axis = 1)\n",
    "y_bootstrap_sample = bootstrap_sample['Class']\n",
    "X_train_bootstrap, X_test_bootstrap, y_train_bootstrap, y_test_bootstrap = train_test_split(X_bootstrap_sample, y_bootstrap_sample, test_size=0.3, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "y_pred = model.predict(X_test_bootstrap)\n",
    "accuracy_xg = accuracy_score(y_test_bootstrap, y_pred)\n",
    "print(\"Accuracy of bootstrap Sampling using XG-Boost:\", accuracy_xg)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "model.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "y_pred = model.predict(X_test_bootstrap)\n",
    "accuracy_rf = accuracy_score(y_test_bootstrap, y_pred)\n",
    "print(\"Accuracy of bootstrap Sampling using Random_forest:\", accuracy_rf)\n",
    "\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMClassifier(verbose=0,force_col_wise=True)\n",
    "model.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "y_pred = model.predict(X_test_bootstrap)\n",
    "accuracy_lgbm = accuracy_score(y_test_bootstrap, y_pred)\n",
    "print(\"Accuracy of Simple Random Sampling using LGBM:\", accuracy_lgbm)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model= DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "y_pred = model.predict(X_test_bootstrap)\n",
    "accuracy_dt = accuracy_score(y_test_bootstrap, y_pred)\n",
    "print(\"Accuracy of bootstrap Sampling using decision_tree:\", accuracy_dt)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "y_pred = model.predict(X_test_bootstrap)\n",
    "accuracy_lr = accuracy_score(y_test_bootstrap, y_pred)\n",
    "print(\"Accuracy for bootstrap sampling using Logistic_Regression:\", accuracy_lr)\n",
    "\n",
    "list5=[accuracy_lr,accuracy_dt,accuracy_lgbm,accuracy_rf,accuracy_xg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame({\n",
    "    'Random_Sampling':list1,\n",
    "    'Systematic':list2,\n",
    "    'Stratified':list3,\n",
    "    'Cluster':list4,\n",
    "    'Bootstrap':list5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.index=['logistic-regression','decision-tree','LGBM','Random_forest','XG-Boost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Sampling</th>\n",
       "      <th>Systematic</th>\n",
       "      <th>Stratified</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Bootstrap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic-regression</th>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.891775</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision-tree</th>\n",
       "      <td>0.956897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.974138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.974138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_forest</th>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.991379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XG-Boost</th>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.965368</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.948276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Random_Sampling  Systematic  Stratified   Cluster  \\\n",
       "logistic-regression         0.887931    0.750000    0.891775  0.948276   \n",
       "decision-tree               0.956897    1.000000    0.952381  0.965517   \n",
       "LGBM                        0.974138    0.416667    0.987013  0.991379   \n",
       "Random_forest               0.982759    0.833333    0.987013  0.991379   \n",
       "XG-Boost                    0.965517    0.833333    0.965368  0.965517   \n",
       "\n",
       "                     Bootstrap  \n",
       "logistic-regression   0.931034  \n",
       "decision-tree         0.974138  \n",
       "LGBM                  0.974138  \n",
       "Random_forest         0.991379  \n",
       "XG-Boost              0.948276  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum value is 1.0 in the row named 'decision-tree' and the column named 'Systematic'.\n"
     ]
    }
   ],
   "source": [
    "max_value_row_index = result.values.argmax() // result.shape[1]\n",
    "\n",
    "# Find the column index where the maximum value occurs\n",
    "max_value_col_index = result.values.argmax() % result.shape[1]\n",
    "\n",
    "# Get the corresponding row and column names\n",
    "max_value_row = result.index[max_value_row_index]\n",
    "max_value_column = result.columns[max_value_col_index]\n",
    "\n",
    "# Get the maximum value itself\n",
    "max_value = result.values.max()\n",
    "\n",
    "# Print the results\n",
    "print(f\"The maximum value is {max_value} in the row named '{max_value_row}' and the column named '{max_value_column}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
